Generated Responses:
PDF: radford2018improving, Response: For a more detailed response, please refer to Appendix A
I am not sure if you have referred to my previous response. Here are some more details: In the
previous response I mentioned how word embedding  is trained with unlabelled data and I referred
to the works of  [5, 6] which used unsupervision to train word representation. My question is
whether the representations are initialized with pre trained word representations or not. I am
curious about the difference between the two. Moreover, the authors in [6] have shown that
initializing word vectors with the word vector of a similar text is more effective than random
vector initialization for word similarity task. So, I wanted to know whether the  word  representations
are initialized from the same  text. If not then how they are learned. Thanks in advance. 


Ans: As far as I know, we use the most recent pre training techniques on the development set
which were presented in the paper [56]. Since it is very hard to obtain the full training corpus
in the open world scenario, in order to get the best pre learning, it was used the data which
could be easily obtained from similar corporal.

PDF: radford2018improving, Response: We can learn many things from text that aren’t directly related to NER. For instance, if we want to create a “super-intelligent”
human, then we need to model the context of the text we‘re reading. This context is not directly
related to language but it can be used to make decisions about what we should do. If we are trying to read
“Gus‚„‰‡›‹ ‭‥ ″‱′‴‵‶․…‣‖‧‼ ‽ ‾‏‐‑       ‌‎​  ‒‍—‟‛‬‪‮‿ ※‫•†‸‗‷―‎�– �‚過！｀ﾞ￬ﻄ﹤️ﺟﱠﳻﲥﶝ﷢﮾ﯜﰰ﵎
The text is usually very short, and we usually only have a few words to work with.

PDF: radford2018improving, Response: We’ve been interested in the question of how to use unsupervisely labeled data for deep learning. We
have experimented with a variety of deep architectures on different tasks (image classification, NER, etc.),
and have found that using the transformer model can help improve the quality of a prediction. Our
approach is in line with the work of Howard et. al [1]. However we don‘t use any preprocessing, but
instead we use preprocessed text as input, and train our models on the entire corpus. The main idea
behind this is that we want to make a model that can predict the output of any given input without
understanding the context of that input. Specifically, a transformer can learn a sentence representation
with information from a wide variety and rich context (i.e. the whole corpus) and only needs to learn
representations for the words that appear in that sentence. On the other hand, if we had used a very
simple preprocessor, like a one-hot encoding of words, then the model would learn only a single
vector representation for each word, instead capturing the surrounding context. Thus, this approach
would be similar to using only the word embedding from the training corpus, rather than using it in
the first place.

PDF: radford2018improving, Response: Please provide the following explanation for the difference between
semi- and un-semisupervised training. I am not asking for a specific
formulation. Is there a difference?
Answer: There is a large amount of literature that discusses uni- or
multi-modal unlabeled learning (or unlabelled learning). However, most of it focuses on how to
train a model to do unter-labelled classification, or how one should use unlables to train a
classi
．ｱﾝ﹀￣ﻳ﷚
I do not think that unidirectional transfer of language models is
important. What is important is to use them to learn language, and then
transfer the learniing from the language to other tasks, such as reasoning,
composing and so on. If we are going to apply this to reasoning
(for example, on reasoning about the meaning of an argument), then we
should be able to get the benefits of unstructured unlabled
data. However we should not rely on it for training, since it may not
be appropriate for reasoning tasks to start with language. This is because
reasoning is, by its nature, more complex than unformatted text.

PDF: radford2018improving, Response: 1. What is a preprocessing step?
2. How do you think the preprocessed data would be used for text classification? What are the
main advantages of using preprocessor as a method to handle data? (e.g. reducing the amount of
labels/data)
3. Explain the main steps of our proposed model. (i.e., word2vec, self-attention, etc.)
4. Which loss function is used during training? How does it help in improving the performance?

